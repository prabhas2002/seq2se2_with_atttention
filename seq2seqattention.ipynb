{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgEJo7FOQtcm",
        "outputId": "451e3483-bdab-4920-ed56-b17accc79cc9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned lines have been written to Assignment2_train_cleaned.txt\n",
            "Cleaned lines have been written to Assignment2_validation_cleaned.txt\n",
            "Cleaned lines have been written to Assignment2_est_cleaned.txt\n"
          ]
        }
      ],
      "source": [
        "def clean_file(input_file_path, output_file_path, has_target=True):\n",
        "    \"\"\"\n",
        "    Cleans the input file by removing start and end ' tokens and ensuring no space before the target text.\n",
        "    Writes the cleaned lines to the output file.\n",
        "\n",
        "    Parameters:\n",
        "    - input_file_path: str, path to the input file\n",
        "    - output_file_path: str, path to the output file\n",
        "    - has_target: bool, indicates if the file contains target text (default is True)\n",
        "    \"\"\"\n",
        "    # Read the file and process each line\n",
        "    with open(input_file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Remove the start and end ' tokens and ensure no space before the target text\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        # Remove start and end ' tokens\n",
        "        line = line.replace(\"'\", \"\").strip()\n",
        "        # Split the line into input and target parts if target exists\n",
        "        if has_target:\n",
        "            parts = line.split(',')\n",
        "            if len(parts) == 2:\n",
        "                input_part = parts[0].strip()\n",
        "                target_part = parts[1].strip()\n",
        "                cleaned_line = f\"{input_part},{target_part}\"\n",
        "                cleaned_lines.append(cleaned_line)\n",
        "        else:\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "    # Write the cleaned lines to a new file\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        for line in cleaned_lines:\n",
        "            file.write(line + '\\n')\n",
        "\n",
        "    print(f\"Cleaned lines have been written to {output_file_path}\")\n",
        "\n",
        "# Define the input and output file paths for training, validation, and test datasets\n",
        "train_input_file_path = 'Assignment2_train.txt'\n",
        "train_output_file_path = 'Assignment2_train_cleaned.txt'\n",
        "\n",
        "validation_input_file_path = 'Assignment2_validation.txt'\n",
        "validation_output_file_path = 'Assignment2_validation_cleaned.txt'\n",
        "\n",
        "test_input_file_path = 'Assignment2_Test.txt'\n",
        "test_output_file_path = 'Assignment2_est_cleaned.txt'\n",
        "\n",
        "# Clean the training, validation, and test files\n",
        "clean_file(train_input_file_path, train_output_file_path, has_target=True)\n",
        "clean_file(validation_input_file_path, validation_output_file_path, has_target=True)\n",
        "clean_file(test_input_file_path, test_output_file_path, has_target=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4h321lvQuU-",
        "outputId": "dbbc5a6b-b8cd-4ba1-d750-f619e1e507b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           0           1\n",
            "0               march 8 1758  1758-03-08\n",
            "1           17 february 1709  1709-02-17\n",
            "2                13 may 1786  1786-05-13\n",
            "3               17 june 1626  1626-06-17\n",
            "4               july 25 1851  1851-07-25\n",
            "...                      ...         ...\n",
            "35995            jun 16 2050  2050-06-16\n",
            "35996   tuesday july 22 1524  1524-07-22\n",
            "35997       28 december 1870  1870-12-28\n",
            "35998  sun 1655 19 september  1655-09-19\n",
            "35999       1581 12 november  1581-11-12\n",
            "\n",
            "[36000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "#reading data\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/data4/home/prabhasreddy/DL_NLP/Assignment-2/Assignment2_train_cleaned.txt', sep=\",\", header=None)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ESMHwzjLSSbR"
      },
      "outputs": [],
      "source": [
        "df.columns = ['dates','translated_dates']\n",
        "\n",
        "# For validation data\n",
        "df_valid = pd.read_csv('/data4/home/prabhasreddy/DL_NLP/Assignment-2/Assignment2_Test_cleaned.txt', sep=\",\", header=None)\n",
        "df_valid.columns = ['dates']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dates</th>\n",
              "      <th>translated_dates</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>march 8 1758</td>\n",
              "      <td>1758-03-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17 february 1709</td>\n",
              "      <td>1709-02-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13 may 1786</td>\n",
              "      <td>1786-05-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17 june 1626</td>\n",
              "      <td>1626-06-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>july 25 1851</td>\n",
              "      <td>1851-07-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              dates translated_dates\n",
              "0      march 8 1758       1758-03-08\n",
              "1  17 february 1709       1709-02-17\n",
              "2       13 may 1786       1786-05-13\n",
              "3      17 june 1626       1626-06-17\n",
              "4      july 25 1851       1851-07-25"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZEHOu753R4Ij"
      },
      "outputs": [],
      "source": [
        "#built vocab\n",
        "src_vocab={}\n",
        "src_vocab['<st>']=0\n",
        "src_vocab['<end>']=1\n",
        "src_vocab['<pad>']=2\n",
        "k=3\n",
        "for i in df['dates']:\n",
        "  for j in i:\n",
        "    if j not in src_vocab:\n",
        "      src_vocab[j]=k\n",
        "      k+=1\n",
        "\n",
        "tgt_vocab = {}\n",
        "tgt_vocab['<st>']=0\n",
        "tgt_vocab['<end>']=1\n",
        "tgt_vocab['<pad>']=2\n",
        "k=3\n",
        "for i in df['translated_dates']:\n",
        "  for j in i:\n",
        "    if j not in tgt_vocab:\n",
        "      tgt_vocab[j]=k\n",
        "      k+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<st>': 0,\n",
              " '<end>': 1,\n",
              " '<pad>': 2,\n",
              " '1': 3,\n",
              " '7': 4,\n",
              " '5': 5,\n",
              " '8': 6,\n",
              " '-': 7,\n",
              " '0': 8,\n",
              " '3': 9,\n",
              " '9': 10,\n",
              " '2': 11,\n",
              " '6': 12,\n",
              " '4': 13}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgt_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gft7Gv860kTQ"
      },
      "outputs": [],
      "source": [
        "inverse_src_vocab = {v: k for k, v in src_vocab.items()}\n",
        "inverse_tgt_vocab = {v: k for k, v in tgt_vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "3Rzu50XEThSy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class CustomDatesDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, tgt_vocab):\n",
        "        self.df = df\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.df.iloc[idx, 0]\n",
        "        tgt_text = self.df.iloc[idx, 1]\n",
        "\n",
        "        src_tokens = [self.src_vocab[char] for char in src_text]\n",
        "        tgt_tokens = [self.tgt_vocab[char] for char in tgt_text]\n",
        "\n",
        "        return src_tokens, tgt_tokens\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    src_max_len = max([len(src) for src in src_batch])\n",
        "    tgt_max_len = max([len(tgt) for tgt in tgt_batch])\n",
        "\n",
        "    src_padded = []\n",
        "    tgt_padded = []\n",
        "\n",
        "    for src, tgt in zip(src_batch, tgt_batch):\n",
        "        src_padded.append([src_vocab['<st>']] + src + [src_vocab['<end>']] + [src_vocab['<pad>']] * (src_max_len - len(src)))\n",
        "        tgt_padded.append([tgt_vocab['<st>']] + tgt + [tgt_vocab['<end>']] + [tgt_vocab['<pad>']] * (tgt_max_len - len(tgt)))\n",
        "\n",
        "    src_padded = torch.tensor(src_padded, dtype=torch.long)\n",
        "    tgt_padded = torch.tensor(tgt_padded, dtype=torch.long)\n",
        "\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "# Build dataloaders\n",
        "train_dataset = CustomDatesDataset(df, src_vocab, tgt_vocab)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "valid_dataset = CustomDatesDataset(df_valid, src_vocab, tgt_vocab)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHZ0ySQMVOKV",
        "outputId": "7cd425b8-c88d-4191-a759-6b39e2b46055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 26]) torch.Size([32, 12])\n"
          ]
        }
      ],
      "source": [
        "for input,output in train_dataloader:\n",
        "  print(input.shape,output.shape)\n",
        "  break\n",
        "\n",
        "# for input,output in valid_dataloader:\n",
        "#   print(input.shape,output.shape)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x-buekuVaQgg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout_layer(self.embedding(src))  # [batch_size, src_len, embed_dim]\n",
        "        outputs, hidden = self.gru(embedded)  # outputs: [batch_size, src_len, hidden_dim], hidden: [num_layers, batch_size, hidden_dim]\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hidden_dim]\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_dim]\n",
        "        \n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hidden_dim]\n",
        "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
        "        return F.softmax(attention, dim=1)  # [batch_size, src_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, attention, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: [batch_size]\n",
        "        # hidden and encoder_outputs are from the encoder initially\n",
        "        # hidden: [num_layers, batch_size, hidden_dim]\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_dim]\n",
        "        \n",
        "        input = input.unsqueeze(1)  # [batch_size, 1]\n",
        "        embedded = self.dropout_layer(self.embedding(input))  # [batch_size, 1, embed_dim]\n",
        "        \n",
        "        attn_weights = self.attention(hidden[-1], encoder_outputs)  # [batch_size, src_len]\n",
        "        attn_weights = attn_weights.unsqueeze(1)  # [batch_size, 1, src_len]\n",
        "        \n",
        "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, embed_dim + hidden_dim]\n",
        "        \n",
        "        output, hidden = self.gru(rnn_input, hidden)  # output: [batch_size, 1, hidden_dim], hidden: [num_layers, batch_size, hidden_dim]\n",
        "        \n",
        "        output = output.squeeze(1)  # [batch_size, hidden_dim]\n",
        "        context = context.squeeze(1)  # [batch_size, hidden_dim]\n",
        "        #not needed , just outputs can also be passed\n",
        "        prediction = self.fc(torch.cat((output, context), dim=1))  # [batch_size, output_dim]\n",
        "        \n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encoder = Encoder(len(src_vocab),128,256,1)\n",
        "\n",
        "# sample_input = torch.randint(0, len(src_vocab), (32, 10))  # (batch_size, sequence_length)\n",
        "# print(\"Input shape:\", sample_input.shape)\n",
        "\n",
        "# # Pass the sample input through the encoder\n",
        "# output, hidden = encoder(sample_input)\n",
        "\n",
        "# # Print the output and hidden shapes\n",
        "# print(\"Output shape:\", output.shape)\n",
        "# print(\"Hidden shape:\", hidden.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #sample input for attention\n",
        "# attention = Attention(256)\n",
        "\n",
        "\n",
        "# encoder_output = torch.randn(32, 12, 256)\n",
        "# dec_hidden = torch.randn(32, 256)\n",
        "# outputs = attention(dec_hidden, encoder_output)\n",
        "# #outputs will be of shape (batch_dim* input_seq_len) whree each row sums to 1\n",
        "# print(outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bCPKKOOYfvIz"
      },
      "outputs": [],
      "source": [
        "# decoder = Decoder(len(tgt_vocab),128,256,1)\n",
        "\n",
        "# sample_input = torch.randint(0, len(tgt_vocab), (32, 2))  # (batch_size, sequence_length)\n",
        "# print(\"Input shape:\", sample_input.shape)\n",
        "# print('encoder hidden shapes:', hidden.shape)\n",
        "# # Pass the sample input through the encoder\n",
        "# predictions,_ = decoder(sample_input,hidden)\n",
        "\n",
        "# # Print the output and hidden shapes\n",
        "# print(\"Output shape:\", predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MhgILXnZix0Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        input = tgt[:, 0]  # <sos> token\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(self, src, tgt_len):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, hidden = self.encoder(src)\n",
        "            input = torch.tensor([0] * src.size(0)).to(self.device)  # <sos> token\n",
        "            generated_sequences = [[] for _ in range(src.size(0))]\n",
        "\n",
        "            for _ in range(tgt_len):\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "                top1 = output.argmax(1)\n",
        "                input = top1\n",
        "                for i in range(src.size(0)):\n",
        "                    generated_sequences[i].append(top1[i].item())\n",
        "\n",
        "            return generated_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wUBOXlWzkcun"
      },
      "outputs": [],
      "source": [
        "# encodersample = Encoder(len(src_vocab),128,256,1)\n",
        "# decodersample = Decoder(len(tgt_vocab),128,256,1)\n",
        "# seq2seq = Seq2Seq(encodersample,decodersample)\n",
        "\n",
        "# sample_encoder_input = torch.randint(0, len(src_vocab), (32, 10))  # (batch_size, sequence_length)\n",
        "# sample_decoder_input = torch.randint(0, len(tgt_vocab), (32, 5))  # (batch_size, sequence_length)\n",
        "\n",
        "# output = seq2seq(sample_encoder_input,sample_decoder_input)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data4/home/prabhasreddy/miniconda/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = len(src_vocab)  # Example input dimension\n",
        "output_dim = len(tgt_vocab)  # Example output dimension\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "dropout = 0.2\n",
        "num_epochs = 12\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "attention = Attention(hidden_dim)\n",
        "encoder = Encoder(input_dim, embed_dim, hidden_dim, num_layers, dropout)\n",
        "decoder = Decoder(output_dim, embed_dim, hidden_dim, num_layers, attention, dropout)\n",
        "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n",
        "optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4jvV3NXEoGKr",
        "outputId": "7f805878-ce99-4434-fb0b-b7af45e1398c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12, Loss: 0.12659820778564446\n",
            "Epoch 2/12, Loss: 0.01156341396137658\n",
            "Epoch 3/12, Loss: 0.011178980942634451\n",
            "Epoch 4/12, Loss: 0.010212213340525826\n",
            "Epoch 5/12, Loss: 0.010279141996573243\n",
            "Epoch 6/12, Loss: 0.009868463377526495\n",
            "Epoch 7/12, Loss: 0.00936404489133727\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_647666/3189400516.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/cs285/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m~/miniconda/envs/cs285/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/cs285/lib/python3.9/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    seq2seq.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = seq2seq(src, tgt)\n",
        "\n",
        "        # Output shape: [batch_size, tgt_len, output_dim]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_dataloader)}')\n",
        "    if epoch % 3 == 0:\n",
        "        torch.save(seq2seq.state_dict(), f'seq2seq_{epoch}.pt')\n",
        "        \n",
        "print('Training Finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save the model and load\n",
        "torch.save(seq2seq.state_dict(), 'seq2seq.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1143642/2133457499.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  seq2seq.load_state_dict(torch.load('seq2seq_6.pt'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
        "seq2seq.load_state_dict(torch.load('seq2seq.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wKx-0Wfv3-PN"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "max_tgt_len = 12\n",
        "predictions = []\n",
        "seq2seq.eval()\n",
        "seq2seq = seq2seq.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "for src in valid_dataloader:\n",
        "    src = src.to(device)\n",
        "    \n",
        "    outputs = seq2seq.inference(src, max_tgt_len)\n",
        "    \n",
        "    # Apply inverse vocab\n",
        "    for output in outputs:\n",
        "        pred = ''\n",
        "        for token in output:\n",
        "            if inverse_tgt_vocab[token] == '<st>':\n",
        "                pred += '0'\n",
        "            elif inverse_tgt_vocab[token] == '<end>':\n",
        "                break\n",
        "            elif inverse_tgt_vocab[token] == '<pad>':\n",
        "                continue\n",
        "            else:\n",
        "                pred += inverse_tgt_vocab[token]\n",
        "               \n",
        "        predictions.append(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_-b4yfeq_5SK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1733-08-14', '1625-11-24', '1723-01-24']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "references = df_valid['dates'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Evaluation metrics\n",
        "correct =0\n",
        "for i in range(len(df_valid)):\n",
        "    reference = references[i]\n",
        "    candidate = predictions[i]\n",
        "    \n",
        "    if reference == candidate:\n",
        "        correct+=1\n",
        "\n",
        "\n",
        "accuracy = correct/len(df_valid)\n",
        "print('Accuracy on Validation data:',accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excat matches :  3861\n",
            "Less than 10 :  0\n",
            "Exact Match Error:  3.4749999999999943\n",
            "Mismatch Error:  0.445\n",
            "Highest Error:  2\n",
            "Lowest Error:  3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "calculate Average Validation Set Error in % (using \"Exact Match over all 10 outputs\" as a metric),\n",
        "Average Validation Set Error in % (number of mismatches averaged over all 10 outputs)(since the ouput is always 10 characters long),\n",
        "Numbering the outputs from 1 to 10 (1 for the most significant digit of the year and 10 for the least significant digit of the date), the validation set error (average number of mismatches) for which output was the highest?\n",
        "Numbering the outputs from 1 to 10 (1 for the most significant digit of the year and 10 for the least significant digit of the date), the validation set error (average number of mismatches) for which output was the lowest?\n",
        "\"\"\"\n",
        "\n",
        "def calculate_all_errors(actual_outputs, predicted_outputs):\n",
        "    \n",
        "    exact_match_error = 0\n",
        "    mismatch_error = 0\n",
        "    position_errors = [0]*10\n",
        "    \n",
        "    less_than_10 = 0\n",
        "    \n",
        "    for actual, predicted in zip(actual_outputs, predicted_outputs):\n",
        "        \n",
        "        if len(actual) != 10 or len(predicted) != 10:\n",
        "            less_than_10 += 1\n",
        "            continue\n",
        "        \n",
        "        exact_match_error += 1 if actual == predicted else 0\n",
        "        for i in range(10):\n",
        "            mismatch_error += 1 if actual[i] != predicted[i] else 0\n",
        "            position_errors[i] += 1 if actual[i] != predicted[i] else 0\n",
        "            \n",
        "    highest_error = position_errors.index(max(position_errors)) + 1\n",
        "    lowest_error = position_errors.index(min(position_errors)) + 1\n",
        "    \n",
        "    print(\"Excat matches : \", exact_match_error)\n",
        "    print(\"Less than 10 : \", less_than_10)\n",
        "        \n",
        "    exact_match_error = (exact_match_error/len(actual_outputs))*100\n",
        "    mismatch_error = (mismatch_error/(len(actual_outputs)*10))*100\n",
        "    \n",
        "    return exact_match_error, mismatch_error, highest_error, lowest_error\n",
        "\n",
        "exact_match_error, mismatch_error, highest_error, lowest_error = calculate_all_errors(actual_targets, predictions)\n",
        "\n",
        "\n",
        "print('Exact Match Error: ', 100-exact_match_error)\n",
        "print('Mismatch Error: ', mismatch_error)\n",
        "print('Highest Error: ', highest_error)\n",
        "print('Lowest Error: ', lowest_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#end of code"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
